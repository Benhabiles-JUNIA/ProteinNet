{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# load proteins in the form of point cloud xyz format \n",
    "# and returns train and validation data according to validation_rate\n",
    "\n",
    "# load protein dataset\n",
    "input_directory = './protein_shape'\n",
    "\n",
    "def load_proteins (input_directory, validation_rate):\n",
    "\n",
    "    train_points = []\n",
    "    test_points = []\n",
    "    \n",
    "    file_list = os.listdir(input_directory)\n",
    "    \n",
    "    for file_name in tqdm(file_list):\n",
    "        \n",
    "        # read point cloud\n",
    "        full_path = os.path.join(input_directory, file_name)\n",
    "        pcd = o3d.io.read_point_cloud(full_path)\n",
    "        points = np.asarray(pcd.points)\n",
    "        \n",
    "        # add to list\n",
    "        train_points.append(points)\n",
    "    \n",
    "    \n",
    "    train_points = np.array(train_points)\n",
    "    validation_length = (int)(train_points.shape[0]*validation_rate)\n",
    "    train_length = train_points.shape[0]- validation_length\n",
    "    np.random.shuffle(train_points)\n",
    "    \n",
    "    train_points, test_points = train_points[:train_length,:,:], train_points[train_length:,:,:]\n",
    "    \n",
    "    return train_points, train_points, test_points, test_points\n",
    "\n",
    "\n",
    "train_points, train_labels, test_points, test_labels = load_proteins(input_directory, 0.2)\n",
    "\n",
    "print(train_points.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "print(test_points.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code cell is adapted from https://keras.io/examples/vision/pointnet/\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "#tf.random.set_random_seed(1234)\n",
    "\n",
    "def augment(points, labels):\n",
    "    # jitter points\n",
    "    points += tf.random.uniform(points.shape, -0.005, 0.005, dtype=tf.float64)\n",
    "    # shuffle points maybe it is not necessary\n",
    "    points = tf.random.shuffle(points)\n",
    "    labels = points\n",
    "    return points, labels\n",
    "\n",
    "\n",
    "def conv_bn(x, filters):\n",
    "    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\")(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
    "    return layers.Activation(\"tanh\")(x)\n",
    "\n",
    "\n",
    "def dense_bn(x, filters):\n",
    "    x = layers.Dense(filters)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
    "    return layers.Activation(\"tanh\")(x)\n",
    "\n",
    "class OrthogonalRegularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, num_features, l2reg=0.001):\n",
    "        self.num_features = num_features\n",
    "        self.l2reg = l2reg\n",
    "        self.eye = tf.eye(num_features)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = tf.reshape(x, (-1, self.num_features, self.num_features))\n",
    "        xxt = tf.tensordot(x, x, axes=(2, 2))\n",
    "        xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n",
    "        return tf.reduce_sum(self.l2reg * tf.square(xxt - self.eye))\n",
    "    \n",
    "def tnet(inputs, num_features):\n",
    "    \n",
    "    # Initalise bias as the indentity matrix\n",
    "    bias = keras.initializers.Constant(np.eye(num_features).flatten())\n",
    "    reg = OrthogonalRegularizer(num_features)\n",
    "    \n",
    "    x = conv_bn(inputs, 32)\n",
    "    x = conv_bn(x, 64)\n",
    "    x = conv_bn(x, 512)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = dense_bn(x, 256)\n",
    "    x = dense_bn(x, 128)\n",
    "    x = layers.Dense(\n",
    "        num_features * num_features,\n",
    "        kernel_initializer=\"zeros\",\n",
    "        bias_initializer=bias,\n",
    "        activity_regularizer=reg,\n",
    "    )(x)\n",
    "    feat_T = layers.Reshape((num_features, num_features))(x)\n",
    "    # Apply affine transformation to input features\n",
    "    return layers.Dot(axes=(2, 1))([inputs, feat_T])\n",
    "\n",
    "\n",
    "NUM_POINTS = 2048\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_points, train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_points, test_labels))\n",
    "\n",
    "# data augmentation\n",
    "train_dataset = train_dataset.shuffle(len(train_points)).map(augment).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.shuffle(len(test_points)).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "inputs = keras.Input(shape=(NUM_POINTS, 3))\n",
    "x = tnet(inputs, 3)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=x, name=\"pointnet\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to train a new model or skip it to use a model already trained\n",
    "model.compile(\n",
    "    loss=\"cosine_similarity\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('./pointnet_weights.hdf5',\n",
    "                                   monitor='val_loss', save_weights_only=True, save_best_only=True)\n",
    "\n",
    "model.fit(train_dataset, epochs=50, validation_data=test_dataset, callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test the perfomance of the model\n",
    "# Specify the model to use\n",
    "model.load_weights('./pointnet_weights.hdf5')\n",
    "model.compile(\n",
    "    loss=\"cosine_similarity\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.evaluate(test_points, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to extract features\n",
    "model_feature_512 = keras.Model(inputs=model.input, outputs=model.get_layer('global_max_pooling1d').output)\n",
    "model_feature_256 = keras.Model(inputs=model.input, outputs=model.get_layer('batch_normalization_3').output)\n",
    "model_feature_128 = keras.Model(inputs=model.input, outputs=model.get_layer('batch_normalization_4').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read query proteins one by one and transform in features 512, 256, 128, save them in txt file\n",
    "input_directory = './queries_protein_shape'\n",
    "\n",
    "file_list = os.listdir(input_directory)\n",
    "\n",
    "file_list= sorted(file_list, key=lambda x: int(x[:-10]))\n",
    "\n",
    "list_points =[]\n",
    "\n",
    "for file_name in tqdm(file_list):\n",
    "    #print(file_name)\n",
    "    # read point cloud\n",
    "    full_path = os.path.join(input_directory, file_name)\n",
    "    pcd = o3d.io.read_point_cloud(full_path)\n",
    "    points = np.asarray(pcd.points)\n",
    "        \n",
    "    # add to list\n",
    "    list_points.append(points)\n",
    "    \n",
    "\n",
    "proteins = np.array(list_points)\n",
    "print(proteins.shape)\n",
    "\n",
    "\n",
    "#extract features\n",
    "queries_features_128 = model_feature_128.predict(proteins)\n",
    "print(queries_features_128.shape)\n",
    "\n",
    "queries_features_256 = model_feature_256.predict(proteins)\n",
    "print(queries_features_256.shape)\n",
    "\n",
    "queries_features_512 = model_feature_512.predict(proteins)\n",
    "print(queries_features_512.shape)\n",
    "\n",
    "#save queries features to txt files\n",
    "np.savetxt('./queries_features_128.txt', queries_features_128)\n",
    "np.savetxt('./queries_features_256.txt', queries_features_256)\n",
    "np.savetxt('./queries_features_512.txt', queries_features_512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset proteins one by one and transform in features 512, 256, 128, save them in txt file\n",
    "input_directory = './protein_shape'\n",
    "\n",
    "file_list = os.listdir(input_directory)\n",
    "\n",
    "file_list= sorted(file_list, key=lambda x: int(x[:-10]))\n",
    "\n",
    "list_points =[]\n",
    "\n",
    "for file_name in tqdm(file_list):\n",
    "    #print(file_name)\n",
    "    # read point cloud\n",
    "    full_path = os.path.join(input_directory, file_name)\n",
    "    pcd = o3d.io.read_point_cloud(full_path)\n",
    "    points = np.asarray(pcd.points)\n",
    "        \n",
    "    # add to list\n",
    "    list_points.append(points)\n",
    "    \n",
    "\n",
    "proteins = np.array(list_points)\n",
    "print(proteins.shape)\n",
    "\n",
    "\n",
    "#extract features\n",
    "features_128 = model_feature_128.predict(proteins)\n",
    "print(features_128.shape)\n",
    "\n",
    "features_256 = model_feature_256.predict(proteins)\n",
    "print(features_256.shape)\n",
    "\n",
    "features_512 = model_feature_512.predict(proteins)\n",
    "print(features_512.shape)\n",
    "\n",
    "#save dataset features to txt files\n",
    "np.savetxt('./features_128.txt', features_128)\n",
    "np.savetxt('./features_256.txt', features_256)\n",
    "np.savetxt('./features_512.txt', features_512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate dissimilarity matrices using euclidean distances\n",
    "# for the ten queries to the 554 shapes\n",
    "# and save to binary file\n",
    "def dissimimarity_function(queries_features, features):\n",
    "    \n",
    "    dissim = []\n",
    "    \n",
    "    for i in range(queries_features.shape[0]):\n",
    "        \n",
    "        dist = (features - queries_features[i])**2\n",
    "        dist = np.sum(dist, axis=1)\n",
    "        dist = np.sqrt(dist)\n",
    "        #dist = dist.reshape(1,554)\n",
    "        # append distance of query i to all 554 proteins\n",
    "        dissim.append(dist)\n",
    "        \n",
    "    dissim = np.array(dissim)\n",
    "    return dissim\n",
    "    \n",
    "\n",
    "dissim_128 = dissimimarity_function(queries_features_128, features_128)\n",
    "#print(dissim_128)\n",
    "\n",
    "dissim_256 = dissimimarity_function(queries_features_256, features_256)\n",
    "#print(dissim_256)\n",
    "\n",
    "dissim_512 = dissimimarity_function(queries_features_512, features_512)\n",
    "#print(dissim_512)\n",
    "\n",
    "\n",
    "# save binary format\n",
    "dissim_128.tofile('./dissim_128.bin')\n",
    "dissim_256.tofile('./dissim_256.bin')\n",
    "dissim_512.tofile('./dissim_512.bin')\n",
    "\n",
    "# save txt format\n",
    "# np.savetxt('dissim_512.txt', dissim_128)\n",
    "# np.savetxt('dissim_512.txt', dissim_256)\n",
    "# np.savetxt('dissim_512.txt', dissim_512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dissimilarity matrices\n",
    "dissim_512 = np.fromfile('./dissim_512.bin', dtype=np.float32) \n",
    "dissim_512 = dissim_512.reshape(10, 554)\n",
    "print(dissim_512)\n",
    "\n",
    "dissim_256 = np.fromfile('./dissim_256.bin', dtype=np.float32) \n",
    "dissim_256 = dissim_256.reshape(10, 554)\n",
    "print(dissim_256)\n",
    "\n",
    "dissim_128 = np.fromfile('./dissim_128.bin', dtype=np.float32) \n",
    "dissim_128 = dissim_128.reshape(10, 554)\n",
    "print(dissim_128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-CPU",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
